# Probability and Statistics
This is a side note providing background on some of fundamental concepts on probability and statistics that are discussed in the main notes. 
### Joint probability distribution
Join probability distribution is the probability distribution of multiple radom variables occurring at the same time. If there are two variables x and y, the join probability distribution is represented by $P(X,Y)$ which provides the probability of different combination of values that x and y could take on.
### Conditional probability distribution
The conditional probability distribution (CPD) refers to the likelihood of observing certain sets of data given other sets of data.
### Likelihood function
Likelihood function is the conditional probability of observing a specific set of data given a certain sets of parameters (e.g., weight parameters in a linear regression model) within a statistical model (e.g., linear regression hypothesis function), which is represented by $P(x|θ)$ where x is the random variable and θ is the parameter in the statistical model.

To better understand the likelihood function let's consider a dataset with one independent variable (x) and one dependent variable (y). The goal is to fit a linear regression model to map x to y. In the context of linear regression, the joint probability distribution and the conditional probability distribution play important roles. 

According to Bayes' theorem, the probability of observing y given x and the statistical model (also called likelihood function) is represented as P(Y | X, parameters) = P(X, Y | parameters) / P(X | parameters). The joint probability distribution, P(X, Y | parameters), accounts for the assumptions made by linear regression, such as linearity, independence, and the Gaussian distribution of errors. It captures the simultaneous occurrence of different combinations of x and y based on these assumptions.

On the other hand, the conditional probability distribution, P(X | parameters), represents the probability of x given the linear regression parameters. In linear regression, x is assumed to be deterministic, meaning it does not have a distribution itself.

To find the optimal values for the parameters in linear regression, we aim to maximize the likelihood function. This involves utilizing optimization algorithms like gradient descent, which helps identify the parameter values that maximize the probability of observing y given x.

In practice, it is often more convenient to work with the log likelihood function due to its numerical stability and computational efficiency.

Let's go through these steps with a simple example to further clarify the principles:

x: [2, 3, 4]
y: [100, 120, 130]

- Joint Probability Distribution:  
The joint probability distribution represents the assumptions made by linear regression. In this case, we assume a linear relationship between x and y with Gaussian errors.
- Hypothesis function:   
y = β0 + β1*x
- Likelihood function:  
L(β0, β1, σ | X, Y) = Π [1 / √(2πσ^2)] * exp[-(Y - (β0 + β1*X))^2 / (2σ^2)]

- let's assume initial parameter values for bias, weight coefficient, and variance of error as β0 = 0, β1 = 0, σ = 1. The likelihood value is:   
L(0, 1, 1 | [2, 3, 4], [100, 120, 130]) = [1 / √(2π)] * exp[-(100 - (0 + 2))^2 / (2^2)] * [1 / √(2π)] * exp[-(120 - (0 + 3))^2 / (2^2)] * [1 / √(2π)] * exp[-(130 - (0 + 4))^2 / (2^2)]

The next step is to iteratively compute the likelihood values for other parameters and find those that yield to highest likelihood value.