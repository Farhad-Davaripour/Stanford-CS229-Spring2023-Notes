# Part 2: Deep Learning
This part covers an overview of neural network, vectorization, and backpropagation.
## Supervised learning with nonlinear models
Neural network based models are considered nonlinear algorithms. As for regression problems using neural networks, the mean squared error (MSE) loss function can still be utilized. While, for classification tasks, the log likelihood loss function is commonly employed. In both cases, an optimization algorithm is necessary to find the optimal weight parameters. However, due to the nonlinearity of the hypothesis function, direct determination of the gradients is not feasible. Therefore, backpropagation (also known as `auto-differentiation`) algorithm, which leverages the chain rule of calculus, is widely employed to compute the gradients efficiently. Backpropagation enables the network to efficiently adjust its weight parameters by propagating the gradients backward through the network.
Neural networks can consist of multiple layers with high nonlinearity, achieved through the introduction of activation functions like ReLU (`Rectified Linear Unit`). This allows the network to capture complex underlying structures in the data by learning hierarchical representations.  
Another significant advantage of neural networks is their ability to automatically learn features or representations from the data, eliminating the need for manual feature engineering by domain experts.  
Additionally, the use of vectorization in neural network computations, which takes advantage of matrix algebra and optimized numerical linear algebra libraries, is crucial for efficient computation. This becomes particularly important when dealing with high-dimensional inputs and large datasets.
### Modern Neural network models
The MLP (`Multi Layer Perceptron`) is one of the basic type of neural network with the architecture composing of input, hidden, and output layer, where the role of hidden layers is to discover and capture complex features and pattern within the data. As for nonlinearity, each neuron within the layer could involve an activation function which transform the weighted sum of it's inputs. The role of the activation function is to incorporate nonlinearity to the network to better represent nonlinear underlying structure of the data.  

In a very deep neural network, the vanishing gradient problem can occur when gradients become very small due to repeated multiplication of small gradients across multiple layers. This can result in negligible gradients, causing the weight parameters to remain unchanged and preventing effective learning from the training data. To address this issue, the `residual connection` technique, also known as `skip connections`, is used. By introducing shortcut connections that bypass certain layers, the gradients can flow more directly and efficiently during backpropagation. These skip connections enable the gradient to propagate through the network without being diminished by the multiplication of small gradients in intermediate layers.
### Backpropagation
The fundamental technique in backpropagation is the chain rule in calculation enabling us to compute the partial derivatives of a composite function, such as J = f(g(z)), by multiplying the partial derivatives of the outer function (J) with respect to the intermediate variable (u) and the partial derivatives of the intermediate variable (u) with respect to the inner variable (z). The result is the partial derivative of J with respect to z, as expressed by ∂J/∂z = (∂J/∂u) * (∂u/∂z). This formula represents the application of the chain rule and enables us to compute the gradient efficiently during backpropagation in order to optimize the parameters of the neural network.