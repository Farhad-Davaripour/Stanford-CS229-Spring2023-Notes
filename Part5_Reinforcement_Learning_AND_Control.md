# Part 5: Reinforcement Learning and Control
Reinforcement learning (RL) is a type of machine learning which operates in an environment where direct supervision isn't possible. The `learning agent` interacts with its environment and receives rewards or penalties. The goal is to learn a strategy that maximizes these rewards over time, learning from its interactions. Furthermore, the problem is mathematically framed using `Markov Decision Processes` (MDPs), which capture the dynamics of the environment, the possible states, actions, and the associated rewards. 

The MDO incudes stations, actions, `state transition probability`, `discount factor`, and `reward function`. The flow starts by having an initial state which changes by taking an action based on state transition probability and then the reward is calculated based on state transition. The payoff is a combination of reward due to action and state transition and the discount factor which aims to amplify the important of immediate reward compared to long term reward.

The actions are generated by a `policy function` which maps state to action based on the action that provides the highest expected sum of discounted reward. In order to find the optimal policy, a value function is defined (as described by the `Bellman equations`) which determines the policy that yields the highest expected sum of discounted rewards. The optimal policy is the one that maximizes the expected payoff for all states regardless of the initial state.

In order to find the optimal policy function, two common approaches are used: value iteration and policy iteration. Value iteration seeks to find the optimal policy function indirectly. It does this by repeatedly updating the value function until it converges, then the policy is updated based on the converged value function. On the other hand, policy iteration directly iterates on the policy function. This approach alternates between computing the value function for a given policy and updating the policy based on the determined value function, until both converge to an optimal solution.

If the state transition probability of reward function are unknown, then we can estimate the transition probability by running trials and obtain the ratio based on how many times the agent transitioned from s1 to s2 over the total number of trials. The same approach could also be employed to estimate the reward function.